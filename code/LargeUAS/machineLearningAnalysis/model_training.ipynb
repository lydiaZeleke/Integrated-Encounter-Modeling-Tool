{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "\n",
    "def trajectory_cruise_segmentation(track, feasibility):\n",
    "\n",
    "        # Load your data\n",
    "        data = track\n",
    "        matching = True\n",
    "\n",
    "        # Define the signals that you are interested in\n",
    "        speed = data['speed_ft_s'].values\n",
    "        altitude = data['alt_ft_agl'].values\n",
    "\n",
    "        # Standardize the variables to have zero mean and unit variance\n",
    "        epsilon = 1e-8\n",
    "        speed = (speed - np.mean(speed)) / (np.std(speed) + epsilon)  #Epsilon helps prevent division by zero cases\n",
    "\n",
    "        #speed = (speed - np.mean(speed)) / np.std(speed)\n",
    "        altitude = (altitude - np.mean(altitude)) / np.std(altitude)\n",
    "\n",
    "        # Create a composite signal\n",
    "        composite_signal = np.sqrt(speed**2 + altitude**2)\n",
    "\n",
    "        # Define algorithm configuration\n",
    "        algo = rpt.Pelt(model=\"rbf\").fit(composite_signal)\n",
    "\n",
    "        # Detection\n",
    "        result = algo.predict(pen=3)  # You might need to adjust the penalty value\n",
    "        if len(feasibility) != len(result):\n",
    "           #print(f\"Length mismatch: Feasibility length is {len(feasibility)}, but trajectory length is {len(result)}.\")\n",
    "           matching = False\n",
    "\n",
    "\n",
    "        # Create a new column 'segment' and set it initially to 0\n",
    "        data['segment'] = 0\n",
    "        boundaries = [(0, result[0])]\n",
    "        data.loc[0:result[0], 'segment'] = 0\n",
    "        \n",
    "        # Loop over the result to assign each segment a unique ID\n",
    "        for i, val in enumerate(result[:-1]):            \n",
    "            if i == len(result)-2:\n",
    "                boundaries.append((val, result[i+1]-1))\n",
    "            else:\n",
    "                boundaries.append((val, result[i+1]))\n",
    "            data.loc[val:result[i+1], 'segment'] = i+1\n",
    "            \n",
    "\n",
    "        return matching, data\n",
    "        # Save the segmented data back to CSV\n",
    "        # data.to_csv('your_data_file_with_segments.csv', index=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(file_path, scaler, encoder):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path)\n",
    "    trajectory_data = data['flight_dynamics'].apply(lambda x: pd.read_json(x))\n",
    "\n",
    "    trajectory_data[0].drop(['lat_deg', 'down_ft', 'lon_deg', 'theta_rad', 'psi_rad', 'phi_rad', 'segment'], axis=1, inplace=True)\n",
    "\n",
    "    boundaries, trajectory = trajectory_cruise_segmentation(trajectory_data[0])\n",
    "    segment_features = trajectory.groupby('segment').apply(lambda x: process_segment(x, False)).reset_index(drop=True)\n",
    "\n",
    "    #trajectory_data = trajectory_data[0].merge(segment_features, on='segment', how='left')\n",
    "\n",
    "    # Apply Feature Engineering\n",
    "    # Broadcast each feature to all rows within the same segment\n",
    "    for col in segment_features.columns:\n",
    "        trajectory_data[0][col] = np.nan\n",
    "        trajectory_data[0][col] = trajectory['segment'].map(segment_features[col])\n",
    "\n",
    "    \n",
    "    scaled_data = scaler.fit_transform(trajectory_data[0])\n",
    "\n",
    "    trajectory_scaled = pd.DataFrame(scaled_data)          ########### Removing scaling #############\n",
    "\n",
    "    vehicle_type = data['vehicle_type'][0]\n",
    "\n",
    "    # Feasibility\n",
    "    feasibility = data['feasibility'][0]\n",
    "    feasibility = ast.literal_eval(feasibility)\n",
    "    feasibility_temp = []\n",
    "    for (i, seg_feasibility) in enumerate(feasibility):\n",
    "        segment_size = len(trajectory_data[0][trajectory_data[0].segment==i])\n",
    "        feasibility_temp = feasibility_temp + [seg_feasibility] * segment_size\n",
    "        \n",
    "        \n",
    "    feasibility = feasibility_temp\n",
    "\n",
    "    return trajectory_scaled, vehicle_type, feasibility, 241\n",
    "\n",
    "def preprocess_file_per_segment(file_path, scaler, encoder):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Feasibility\n",
    "    feasibility = data['feasibility'][0]\n",
    "    feasibility = ast.literal_eval(feasibility)\n",
    "\n",
    "    #Trajectory\n",
    "    trajectory_data = data['flight_dynamics'].apply(lambda x: pd.read_json(x))\n",
    "\n",
    "    trajectory_data[0].drop(['segment'], axis=1, inplace=True)\n",
    "\n",
    "    matching, trajectory = trajectory_cruise_segmentation(trajectory_data[0], feasibility)\n",
    "    \n",
    "    segment_count = trajectory['segment'].nunique()  # Count unique segments\n",
    "    \n",
    "    # Apply Feature Engineering\n",
    "    engineered_trajectory = trajectory.groupby('segment').apply(lambda x: process_segment(x, True)).reset_index(drop=True)\n",
    "\n",
    "    vehicle_type = data['vehicle_type'][0]\n",
    "\n",
    "    return engineered_trajectory, vehicle_type, feasibility, segment_count, matching\n",
    "\n",
    "def process_segment(group, segmented):\n",
    "    # Ensure the group is sorted by Time\n",
    "    group = group.sort_values(by='Time')\n",
    "\n",
    "    # Calculate time difference\n",
    "    time_elapsed = group['Time'].iloc[-1] - group['Time'].iloc[0]\n",
    "    \n",
    "    airspeed_value = group['speed_ft_s'].mean()\n",
    "    # Calculate distance\n",
    "    distance = airspeed_value * time_elapsed\n",
    "\n",
    "    # Calculate altitude\n",
    "\n",
    "    altitude = group['alt_ft_msl'].mean() #(group['alt_ft_msl'].iloc[-1] + group['alt_ft_msl'].iloc[0])/2\n",
    "    \n",
    "    if segmented:\n",
    "\n",
    "        return pd.Series({'distance': distance, 'airspeed': airspeed_value, 'altitude': altitude})\n",
    "    else:\n",
    "        return pd.Series({'distance': distance, 'airspeed_const': airspeed_value, 'elapsed_time': time_elapsed})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "data_by_vehicle_type = {}\n",
    "\n",
    "# Assuming you know the range of values in your trajectory data and the different vehicle types\n",
    "scaler = StandardScaler() #feature_range=(0, 1)) # Adjust the range according to your data\n",
    "encoder = OneHotEncoder(sparse= False)\n",
    "\n",
    "current_dir = os.getcwd() #os.path.dirname(os.path.abspath())\n",
    "feasibility_file_path = os.path.join(os.path.dirname(current_dir), 'logs', 'feasibility_data')\n",
    "\n",
    "file_paths = [os.path.join(feasibility_file_path, file) for file in os.listdir(feasibility_file_path)]\n",
    "all_trajectories = []\n",
    "all_vehicle_types = []\n",
    "all_feasbilities = []\n",
    "segment_counts = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    trajectory, vehicle_type, feasibility, segment_count, traj_feas_match = preprocess_file_per_segment(file_path, scaler, encoder)\n",
    "    segment_counts[file_path] = segment_count\n",
    "\n",
    "    if vehicle_type not in data_by_vehicle_type:\n",
    "        data_by_vehicle_type[vehicle_type] = []\n",
    "    if traj_feas_match:\n",
    "        data_by_vehicle_type[vehicle_type].append((trajectory, feasibility))\n",
    "        all_trajectories.append(trajectory)\n",
    "        all_vehicle_types.append(vehicle_type)\n",
    "        all_feasbilities.append(feasibility)\n",
    "\n",
    "file_with_max_segments = max(segment_counts, key=segment_counts.get)\n",
    "max_count = segment_counts[file_with_max_segments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pad_trajectory(trajectory, max_length=max_count, padding_value=0.0):\n",
    "    # Calculate the number of time steps to pad\n",
    "    padding_length = max_length - trajectory.shape[0]\n",
    "\n",
    "    # Create the padding array\n",
    "    padding = np.full((padding_length, trajectory.shape[1]), padding_value, dtype='float32')\n",
    "\n",
    "    # Concatenate the trajectory and the padding\n",
    "    padded_trajectory = np.concatenate([trajectory, padding], axis=0)\n",
    "    if padded_trajectory.shape[0] != 241:\n",
    "        print('Not padded correctly ', padded_trajectory.shape[0])\n",
    "\n",
    "    return padded_trajectory\n",
    "\n",
    "def pad_feasibility(feasibility, max_length= max_count, padding_value = False):\n",
    "    \n",
    "    padding_length = max_length - len(feasibility)\n",
    "    padding = np.full((padding_length), padding_value)\n",
    "    padded_feasibility = np.concatenate([feasibility, padding], axis=0)\n",
    "\n",
    "    return padded_feasibility\n",
    "\n",
    "    \n",
    "def create_sequences(trajectories, feasibilities, sequence_length):\n",
    "    X, y = [], []\n",
    "    for trajectory, feasibility in zip(trajectories, feasibilities):\n",
    "        for i in range(len(trajectory) - sequence_length):\n",
    "            seq_trajectory = trajectory[i:i + sequence_length].values\n",
    "            # seq_vehicle_type = np.tile(vehicle_type, (sequence_length, 1))\n",
    "            # seq_combined = np.hstack((seq_trajectory, seq_vehicle_type))\n",
    "            X.append(seq_trajectory)\n",
    "            y.append(feasibility)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def reshape_dataset(trajectories, feasibilities, vehicle_type_encoded):\n",
    "    X, y = [], []\n",
    "\n",
    "    for trajectory, feasibility, vehicle_type in zip(trajectories, feasibilities, vehicle_type_encoded):\n",
    "        \n",
    "        # trajectory_padded = pad_trajectory(trajectory)\n",
    "        # trajectory_feasibility = pad_trajectory(feasibility)\n",
    "\n",
    "        # Tile the encoded vehicle type to match the trajectory time steps\n",
    "        tiled_vehicle_type = np.tile(vehicle_type, (trajectory.shape[0], 1))\n",
    "\n",
    "        # Concatenate the tiled vehicle type with the trajectory\n",
    "        trajectory_with_vehicle_type = np.concatenate([trajectory, tiled_vehicle_type], axis=1)\n",
    "        X.append(trajectory_with_vehicle_type)\n",
    "        y.append(feasibility)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def reshape_dataset_for_traditional_ml(trajectories, feasibilities, vehicle_type_encoded):\n",
    "    X, y = None,  None\n",
    "    # padded_trajectories = list(map(lambda df: pad_trajectory(df), trajectories))\n",
    "    # padded_feasibilities = list(map(lambda df: pad_feasibility(df), feasibilities))\n",
    "    \n",
    "    for trajectory, feasibility, vehicle_type in zip(trajectories, feasibilities, vehicle_type_encoded):\n",
    "\n",
    "        tiled_vehicle_types = np.tile(vehicle_type, (trajectory.shape[0], 1))\n",
    "        tiled_trajectory = np.concatenate([trajectory, tiled_vehicle_types], axis=1)   # Concatenate with the encoded vehicle type\n",
    "        \n",
    "        if len(feasibility) != len(trajectory):\n",
    "            print(f\"Length mismatch: Feasibility length is {len(feasibility)}, but trajectory length is {len(trajectory)}.\")\n",
    "\n",
    "        X = tiled_trajectory if X is None else np.vstack((X, tiled_trajectory))\n",
    "        y = feasibility if y is None else np.concatenate([y, feasibility], axis=0)\n",
    "\n",
    "    X[:, :trajectory.shape[1]] = X[:, :trajectory.shape[1]].astype('float64')\n",
    "    return np.array(X), np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_MLSTM_FCN_split():\n",
    "    \n",
    "    unique_vehicle_types = np.unique(all_vehicle_types)\n",
    "    encoder.fit(unique_vehicle_types.reshape(-1, 1))\n",
    "    encoded_vehicle_types = encoder.transform(np.array(all_vehicle_types).reshape(-1, 1))\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "    X, y = reshape_dataset(all_trajectories, all_feasbilities, encoded_vehicle_types)\n",
    "\n",
    "    X_shuffled = X.copy()\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X_shuffled = X_shuffled[permutation]\n",
    "    y_shuffled = y[permutation]\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.3, random_state=42)\n",
    "    NUM_FEATURES = np.array(X_train).shape[2]\n",
    "    TIMESTEPS = max_count\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train, dtype=int)  # Converts True to 1 and False to 0\n",
    "    y_test = np.array(y_test, dtype=int)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[2], X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[2], X_test.shape[1])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def prepare_data_MLSTM_FCN_kfold():\n",
    "    \n",
    "    unique_vehicle_types = np.unique(all_vehicle_types)\n",
    "    encoder.fit(unique_vehicle_types.reshape(-1, 1))\n",
    "    encoded_vehicle_types = encoder.transform(np.array(all_vehicle_types).reshape(-1, 1))\n",
    "    X, y = reshape_dataset(all_trajectories, all_feasbilities, encoded_vehicle_types)\n",
    "\n",
    "    # Specify the number of folds (k)\n",
    "    n_splits = 5  # You can choose the number of folds\n",
    "\n",
    "    # Initialize StratifiedKFold with the number of splits and random seed (if desired)\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store training and testing indices for each fold\n",
    "    train_indices_list = []\n",
    "    test_indices_list = []\n",
    "\n",
    "    # Split the data into train and test indices for each fold\n",
    "    for train_indices, test_indices in stratified_kfold.split(X, y):\n",
    "        train_indices_list.append(train_indices)\n",
    "        test_indices_list.append(test_indices)\n",
    "\n",
    "    # Using training and testing indices for cross-validation\n",
    "    for fold in range(n_splits):\n",
    "        X_train, X_test = X[train_indices_list[fold]], X[test_indices_list[fold]]\n",
    "        y_train, y_test = y[train_indices_list[fold]], y[test_indices_list[fold]]\n",
    "        \n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train, dtype=int)  # Converts True to 1 and False to 0\n",
    "    y_test = np.array(y_test, dtype=int)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[2], X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[2], X_test.shape[1])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def prepare_data_trad_kfold():\n",
    "    \n",
    "    unique_vehicle_types = np.unique(all_vehicle_types)\n",
    "    encoder.fit(unique_vehicle_types.reshape(-1, 1))\n",
    "    encoded_vehicle_types = encoder.transform(np.array(all_vehicle_types).reshape(-1, 1))\n",
    "    X, y = reshape_dataset_for_traditional_ml(all_trajectories, all_feasbilities, encoded_vehicle_types)\n",
    "  \n",
    "    # Specify the number of folds (k)\n",
    "    n_splits = 10  # You can choose the number of folds\n",
    "\n",
    "    # Initialize StratifiedKFold with the number of splits and random seed (if desired)\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store training and testing indices for each fold\n",
    "    train_indices_list = []\n",
    "    test_indices_list = []\n",
    "\n",
    "    # Split the data into train and test indices for each fold\n",
    "    for train_indices, test_indices in stratified_kfold.split(X, y):\n",
    "        train_indices_list.append(train_indices)\n",
    "        test_indices_list.append(test_indices)\n",
    "\n",
    "    for fold in range(n_splits):\n",
    "        X_train, X_test = X[train_indices_list[fold]], X[test_indices_list[fold]]\n",
    "        y_train, y_test = y[train_indices_list[fold]], y[test_indices_list[fold]]\n",
    "        \n",
    "    # Train  model on X_train and y_train, and evaluate it on X_test and y_test for each fold     \n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train, dtype=int)  # Converts True to 1 and False to 0\n",
    "    y_test = np.array(y_test, dtype=int)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_trad_split():\n",
    "    unique_vehicle_types = np.unique(all_vehicle_types)\n",
    "    encoder.fit(unique_vehicle_types.reshape(-1, 1))\n",
    "    encoded_vehicle_types = encoder.transform(np.array(all_vehicle_types).reshape(-1, 1))\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "    X, y = reshape_dataset_for_traditional_ml(all_trajectories, all_feasbilities, encoded_vehicle_types)\n",
    "    # X_shuffled = X.copy()\n",
    "    # permutation = np.random.permutation(X.shape[0])\n",
    "    # X_shuffled = X_shuffled[permutation]\n",
    "    # y_shuffled = y[permutation]\n",
    "    \n",
    "    pca = PCA(n_components=10)\n",
    "    #X = pca.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train, dtype=int)  # Converts True to 1 and False to 0\n",
    "    y_test = np.array(y_test, dtype=int)\n",
    "    \n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lydiazeleke/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110347, 7)\n",
      "Weighted Precision: 0.8199151236049468\n",
      "Weighted Recall: 0.8201326711857034\n",
      "Weighted F1-Score: 0.8200212671343249\n",
      "Precision: 0.737528041875868\n",
      "Recall: 0.7337655436284408\n",
      "F1-Score: 0.7356419818859883\n",
      "Accuracy: 0.8201326711857034\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "X_train, X_test, y_train, y_test = prepare_data_trad_split()\n",
    "print(X_train.shape)\n",
    "clf = svm.SVC(kernel='rbf')    # or kernel='rbf'\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred,)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision_wgt = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_wgt = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_wgt = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the weighted results\n",
    "print(f'Weighted Precision: {precision_wgt}')\n",
    "print(f'Weighted Recall: {recall_wgt}')\n",
    "print(f'Weighted F1-Score: {f1_wgt}')\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lydiazeleke/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision: 0.8039371758406376\n",
      "Weighted Recall: 0.8073660552454144\n",
      "Weighted F1-Score: 0.8042357863611598\n",
      "Precision: 0.7474820143884892\n",
      "Recall: 0.6602414742639271\n",
      "F1-Score: 0.7011584748622203\n",
      "Accuracy: 0.8073660552454144\n"
     ]
    }
   ],
   "source": [
    "# knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data_trad_kfold()\n",
    "clf = KNeighborsClassifier(n_neighbors = 6) \n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred,)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision_wgt = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_wgt = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_wgt = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the weighted results\n",
    "print(f'Weighted Precision: {precision_wgt}')\n",
    "print(f'Weighted Recall: {recall_wgt}')\n",
    "print(f'Weighted F1-Score: {f1_wgt}')\n",
    "\n",
    "# Print the results\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision: 0.97093815028672\n",
      "Weighted Recall: 0.9709272819546146\n",
      "Weighted F1-Score: 0.9709324404398404\n",
      "Precision: 0.9568527918781726\n",
      "Recall: 0.9582715526371531\n",
      "F1-Score: 0.9575616467351044\n",
      "Accuracy: 0.9709272819546146\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 150) \n",
    "rf_clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred,)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision_wgt = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_wgt = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_wgt = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the weighted results\n",
    "print(f'Weighted Precision: {precision_wgt}')\n",
    "print(f'Weighted Recall: {recall_wgt}')\n",
    "print(f'Weighted F1-Score: {f1_wgt}')\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision: 0.97093815028672\n",
      "Weighted Recall: 0.9709272819546146\n",
      "Weighted F1-Score: 0.9709324404398404\n",
      "Precision: 0.9568527918781726\n",
      "Recall: 0.9582715526371531\n",
      "F1-Score: 0.9575616467351044\n",
      "Accuracy: 0.9709272819546146\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "import xgboost as xgb \n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth = 8, n_estimators = 300)\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred,)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision_wgt = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_wgt = recall_score(y_test, y_pred, average='weighted')\n",
    "f1_wgt = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the weighted results\n",
    "print(f'Weighted Precision: {precision_wgt}')\n",
    "print(f'Weighted Recall: {recall_wgt}')\n",
    "print(f'Weighted F1-Score: {f1_wgt}')\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9494135704647694\n",
      "Recall: 0.9495396215471616\n",
      "F1-Score: 0.9492999583209738\n",
      "Accuracy: 0.9495396215471616\n"
     ]
    }
   ],
   "source": [
    "# fully connected deep neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200,200,200), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Best Performing Model to Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/lydiazeleke/Desktop/encounter_gen_tool/integrated_encounter_tool/LUAS/output/Feasibility_Prediction_Model/random_forest_model.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the training history\n",
    "import pickle\n",
    "import joblib\n",
    "from joblib import load, dump\n",
    "\n",
    "path_to_model = os.path.join(current_dir, 'model', 'Feasibility_Prediction_Model')\n",
    "\n",
    "model_filename = 'random_forest_model.joblib'\n",
    "\n",
    "# Ensure the directory exists, create if it does not\n",
    "if not os.path.exists(path_to_model):\n",
    "    os.makedirs(path_to_model)\n",
    "\n",
    "# Full path to the model file\n",
    "model_path = os.path.join(path_to_model, model_filename)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(rf_clf, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Precision: 0.9716992007277651\n",
      "Recall: 0.9717258128828796\n",
      "F1-Score: 0.9717097690408484\n",
      "Accuracy: 0.9717258128828796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the model file exists\n",
    "if os.path.exists(model_path):\n",
    "    # Load the model\n",
    "    model_loaded = load(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Model file does not exist.\")\n",
    "\n",
    "# Assuming the model loads successfully and you have X_test and X_train ready\n",
    "if 'model_loaded' in locals():\n",
    "    # Evaluate on X_test\n",
    "    y_pred_test = model_loaded.predict(X_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
    "    recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
    "    f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Precision: {precision_test}')\n",
    "    print(f'Recall: {recall_test}')\n",
    "    print(f'F1-Score: {f1_test}')\n",
    "    print(f'Accuracy: {accuracy_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
